{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Training Pipeline\n",
    "\n",
    "quickstart: 一个完整的训练流程演示"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Stage 1: Continue Pretraining\n",
    "\n",
    "第一阶段：PT(Continue PreTraining)增量预训练，在海量领域文本数据上二次预训练GPT模型，以适配领域数据分布\n",
    "\n",
    "注意：\n",
    "1. 此阶段是可选的，如果你没有海量领域文本，可以跳过此阶段，直接进行SFT阶段的有监督微调\n",
    "2. 我实验发现：做领域知识注入，SFT比PT更高效，也可以跳过PT阶段"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 说明：\n",
    "\n",
    "以下 notebook/colab 代码为了快速验证训练代码可用，我们使用了小size的生成模型和小样本数据集，实际使用时，需要使用更大的模型和数据集，以获得更好的效果。\n",
    "\n",
    "1. 生成模型：使用的是Bloom的`bigscience/bloomz-560m`\n",
    "2. 数据集：PT阶段使用的是中文天龙八部小说部分文本和英文书籍部分文本，位于`data/pretrain`文件夹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 配置运行环境\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone --depth 1 https://github.com/shibing624/MedicalGPT.git\n",
    "%cd MedicalGPT\n",
    "%ls\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage1 咱们开始吧\n",
    "\n",
    "训练步骤如下：\n",
    "\n",
    "1. 确认训练集\n",
    "2. 执行训练脚本\n",
    "\n",
    "训练脚本的执行逻辑如下：\n",
    "1. 导入依赖包\n",
    "2. 设置参数\n",
    "3. 定义各函数并加载训练集\n",
    "4. 加载模型和tokenizer\n",
    "5. 开始训练并评估\n",
    "6. 查看训练结果\n",
    "\n",
    "**以下参数可以根据你的GPU实际情况修改，当前参数是根据Colab的T4单卡GPU（16GB显存）配置的**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-03T03:59:09.437932Z",
     "start_time": "2024-07-03T03:59:09.049802Z"
    }
   },
   "source": "%ls ./data/pretrain/",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)\r\n",
      "en_article_tail500.txt  fever.txt  tianlongbabu.txt\r\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-03T04:15:36.368054Z",
     "start_time": "2024-07-03T04:15:36.247165Z"
    }
   },
   "source": [
    "!python pretraining.py \\\n",
    "    --model_type bloom \\\n",
    "    --model_name_or_path bigscience/bloomz-560m \\\n",
    "    --train_file_dir ./data/pretrain \\\n",
    "    --validation_file_dir ./data/pretrain \\\n",
    "    --per_device_train_batch_size 3 \\\n",
    "    --per_device_eval_batch_size 3 \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --use_peft True \\\n",
    "    --seed 42 \\\n",
    "    --fp16 \\\n",
    "    --max_train_samples 20000 \\\n",
    "    --max_eval_samples 10 \\\n",
    "    --num_train_epochs 1 \\\n",
    "    --learning_rate 2e-4 \\\n",
    "    --warmup_ratio 0.05 \\\n",
    "    --weight_decay 0.01 \\\n",
    "    --logging_strategy steps \\\n",
    "    --logging_steps 10 \\\n",
    "    --eval_steps 50 \\\n",
    "    --evaluation_strategy steps \\\n",
    "    --save_steps 500 \\\n",
    "    --save_strategy steps \\\n",
    "    --save_total_limit 3 \\\n",
    "    --gradient_accumulation_steps 1 \\\n",
    "    --preprocessing_num_workers 1 \\\n",
    "    --block_size 128 \\\n",
    "    --group_by_length True \\\n",
    "    --output_dir outputs-pt-v1 \\\n",
    "    --overwrite_output_dir \\\n",
    "    --ddp_timeout 30000 \\\n",
    "    --logging_first_step True \\\n",
    "    --target_modules all \\\n",
    "    --lora_rank 8 \\\n",
    "    --lora_alpha 16 \\\n",
    "    --lora_dropout 0.05 \\\n",
    "    --torch_dtype float16 \\\n",
    "    --device_map auto \\\n",
    "    --report_to tensorboard \\\n",
    "    --ddp_find_unused_parameters False \\\n",
    "    --gradient_checkpointing True"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)\r\n",
      "/bin/bash: python: command not found\r\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "这个脚本的重要参数可以分为几个类别，包括模型配置、训练控制、性能优化和日志记录。以下是每个类别的关键参数提炼：\n",
    "\n",
    "模型配置\n",
    "- `--model_type bloom`: 指定模型类型为Bloom。\n",
    "- `--model_name_or_path bigscience/bloomz-560m`: 使用的预训练模型路径或名称。\n",
    "- `--use_peft True`: 启用PEFT(Progressive Error Feedback Training)训练策略。\n",
    "- `--lora_rank 8` 和 `--lora_alpha 16`: LoRA(Low-Rank Adaptation)的配置参数，用于调整模型的适应性。\n",
    "- `--lora_dropout 0.05`: LoRA层的dropout率。\n",
    "\n",
    "训练控制\n",
    "- `--train_file_dir` 和 `--validation_file_dir`: 指定训练和验证数据集的目录。\n",
    "- `--do_train` 和 `--do_eval`: 启用训练和评估。\n",
    "- `--max_train_samples 20000` 和 `--max_eval_samples 10`: 分别限制训练和评估使用的最大样本数。\n",
    "- `--num_train_epochs 1`: 训练的轮次。\n",
    "- `--learning_rate 2e-4`: 学习率。\n",
    "- `--warmup_ratio 0.05`: 预热比例。\n",
    "- `--weight_decay 0.01`: 权重衰减，用于正则化。\n",
    "\n",
    "性能优化\n",
    "- `--fp16`: 启用半精度浮点数训练，减少内存使用并可能加速训练。\n",
    "- `--gradient_accumulation_steps 1`: 梯度累积步骤，用于在内存限制下增加批量大小。\n",
    "- `--preprocessing_num_workers 1`: 数据预处理的工作线程数。\n",
    "- `--block_size 128`: 输入序列的最大长度。\n",
    "- `--group_by_length True`: 根据序列长度对数据进行分组，以减少填充。\n",
    "\n",
    "日志记录和保存\n",
    "- `--logging_strategy steps` 和 `--logging_steps 10`: 日志记录策略和步骤。\n",
    "- `--eval_steps 50`: 每隔多少步进行一次评估。\n",
    "- `--save_steps 500`: 每隔多少步保存一次模型。\n",
    "- `--output_dir outputs-pt-v1`: 输出目录，用于保存训练结果。\n",
    "- `--overwrite_output_dir`: 如果输出目录已存在，覆盖它。\n",
    "\n",
    "这些参数共同定义了训练的模型类型、数据、训练过程以及性能优化和日志记录的方式。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-03T04:50:03.484496Z",
     "start_time": "2024-07-03T04:50:03.348077Z"
    }
   },
   "source": [
    "%ls -lh outputs-pt-v1"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)\r\n",
      "total 26M\r\n",
      "-rw-r--r-- 1 root root 5.0K Jul  3 12:23 README.md\r\n",
      "-rw-r--r-- 1 root root  699 Jul  3 12:23 adapter_config.json\r\n",
      "-rw-r--r-- 1 root root  13M Jul  3 12:23 adapter_model.safetensors\r\n",
      "-rw-r--r-- 1 root root  472 Jul  3 12:23 all_results.json\r\n",
      "drwxr-xr-x 2 root root 4.0K Jul  3 12:22 \u001B[0m\u001B[01;34mcheckpoint-500\u001B[0m/\r\n",
      "drwxr-xr-x 2 root root 4.0K Jul  3 12:23 \u001B[01;34mcheckpoint-822\u001B[0m/\r\n",
      "-rw-r--r-- 1 root root  263 Jul  3 12:23 eval_results.json\r\n",
      "drwxr-xr-x 3 root root 4.0K Jul  3 12:20 \u001B[01;34mruns\u001B[0m/\r\n",
      "-rw-r--r-- 1 root root  552 Jul  3 12:23 special_tokens_map.json\r\n",
      "-rw-r--r-- 1 root root  14M Jul  3 12:23 tokenizer.json\r\n",
      "-rw-r--r-- 1 root root 1004 Jul  3 12:23 tokenizer_config.json\r\n",
      "-rw-r--r-- 1 root root  229 Jul  3 12:23 train_results.json\r\n",
      "-rw-r--r-- 1 root root  19K Jul  3 12:23 trainer_state.json\r\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-03T05:02:10.664618Z",
     "start_time": "2024-07-03T05:02:10.529149Z"
    }
   },
   "cell_type": "code",
   "source": "%ls -lh outputs-pt-v1/runs",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)\r\n",
      "total 512\r\n",
      "drwxr-xr-x 2 root root 4.0K Jul  3 12:23 \u001B[0m\u001B[01;34mJul03_12-19-31_intern-studio-50002342\u001B[0m/\r\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型训练结果：\n",
    "- 使用lora训练模型，则保存的lora权重是`adapter_model.bin`, lora配置文件是`adapter_config.json`，合并到base model的方法见`merge_peft_adapter.py`\n",
    "- 日志保存在`output_dir/runs`目录下，可以使用tensorboard查看，启动tensorboard方式如下：`tensorboard --logdir output_dir/runs --host 0.0.0.0 --port 8009`"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "TensorBoard 是 TensorFlow 的可视化工具，它允许用户查看训练过程中的各种指标，如损失函数、准确率等，以及模型的图结构和权重直方图。使用 TensorBoard 可以帮助开发者更直观地理解、调试和优化程序。\n",
    "\n",
    "在您提到的脚本中，TensorBoard 被用来查看模型训练的日志。主要关注点包括：\n",
    "\n",
    "1. **训练和验证损失**：观察模型在训练过程中损失函数的变化，帮助判断模型是否在学习和进步。\n",
    "2. **准确率**：如果日志中包含准确率信息，可以观察模型在训练和验证数据集上的表现。\n",
    "3. **学习率**：查看学习率随训练进程的变化，对于使用学习率调度器的训练非常有用。\n",
    "4. **权重和偏置的分布**：TensorBoard 可以展示模型参数（权重和偏置）的分布和变化情况，有助于诊断模型训练过程中的问题，如梯度消失或爆炸。\n",
    "5. **图结构**：查看模型的结构，确保模型的构建符合预期。\n",
    "\n",
    "启动 TensorBoard 的命令如下所示，这个命令会启动一个 Web 服务器，您可以在浏览器中查看训练过程的可视化结果：\n",
    "\n",
    "```bash\n",
    "tensorboard --logdir ooutputs-pt-v1/runs --host 0.0.0.0 --port 8010\n",
    "```\n",
    "\n",
    "- `--logdir` 指定了日志文件的目录。\n",
    "- `--host 0.0.0.0` 允许从任何 IP 地址访问 TensorBoard，增加了访问的灵活性。\n",
    "- `--port 8009` 指定了访问 TensorBoard 的端口。\n",
    "\n",
    "通过查看这些日志，开发者可以更好地理解模型的训练过程，及时发现并解决训练中的问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "lora模型权重合并到base model，合并后的模型保存在`--output_dir`目录下，合并方法如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!python merge_peft_adapter.py --model_type bloom \\\n",
    "    --base_model bigscience/bloomz-560m --lora_model outputs-pt-v1 --output_dir merged-pt/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%ls -lh merged-pt/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%cat merged-pt/config.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stage1 增量预训练完成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-15T13:56:17.081153Z",
     "start_time": "2023-06-15T13:56:17.032821Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Stage 2: Supervised FineTuning\n",
    "\n",
    "第二阶段：SFT(Supervised Fine-tuning)有监督微调，构造指令微调数据集，在预训练模型基础上做指令精调，以对齐指令意图，并注入领域知识\n",
    "\n",
    "| Stage 2: Supervised Fine-tuning | [supervised_finetuning.py](https://github.com/shibing624/MedicalGPT/blob/main/supervised_finetuning.py) | [run_sft.sh](https://github.com/shibing624/MedicalGPT/blob/main/run_sft.sh)  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 说明：\n",
    "以下 notebook/colab 代码为了快速验证训练代码可用，我们使用了小size的生成模型和小样本数据集，实际使用时，需要使用更大的模型和数据集，以获得更好的效果。\n",
    "\n",
    "1. 生成模型：使用的是Bloom的`bigscience/bloomz-560m` 或者 Stage1得到的预训练模型\n",
    "2. 数据集：SFT阶段使用的是使用的是Belle的1千条抽样数据，位于`data/finetune`文件夹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Stage2 咱们开始吧\n",
    "\n",
    "训练步骤如下：\n",
    "\n",
    "1. 确认训练集\n",
    "2. 执行训练脚本\n",
    "\n",
    "训练脚本的执行逻辑如下：\n",
    "1. 导入依赖包\n",
    "2. 设置参数\n",
    "3. 定义各函数并加载训练集\n",
    "4. 加载模型和tokenizer\n",
    "5. 开始训练并评估\n",
    "6. 查看训练结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-15T13:58:38.966506Z",
     "start_time": "2023-06-15T13:58:38.778132Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%ls ./data/finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!python supervised_finetuning.py \\\n",
    "    --model_type bloom \\\n",
    "    --model_name_or_path merged-pt \\\n",
    "    --train_file_dir ./data/finetune \\\n",
    "    --validation_file_dir ./data/finetune \\\n",
    "    --per_device_train_batch_size 4 \\\n",
    "    --per_device_eval_batch_size 4 \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --use_peft True \\\n",
    "    --fp16 \\\n",
    "    --max_train_samples 1000 \\\n",
    "    --max_eval_samples 10 \\\n",
    "    --num_train_epochs 1 \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --warmup_ratio 0.05 \\\n",
    "    --weight_decay 0.05 \\\n",
    "    --logging_strategy steps \\\n",
    "    --logging_steps 10 \\\n",
    "    --eval_steps 50 \\\n",
    "    --evaluation_strategy steps \\\n",
    "    --save_steps 500 \\\n",
    "    --save_strategy steps \\\n",
    "    --save_total_limit 3 \\\n",
    "    --gradient_accumulation_steps 1 \\\n",
    "    --preprocessing_num_workers 1 \\\n",
    "    --output_dir outputs-sft-v1 \\\n",
    "    --overwrite_output_dir \\\n",
    "    --ddp_timeout 30000 \\\n",
    "    --logging_first_step True \\\n",
    "    --target_modules all \\\n",
    "    --lora_rank 8 \\\n",
    "    --lora_alpha 16 \\\n",
    "    --lora_dropout 0.05 \\\n",
    "    --torch_dtype float16 \\\n",
    "    --device_map auto \\\n",
    "    --report_to tensorboard \\\n",
    "    --ddp_find_unused_parameters False \\\n",
    "    --gradient_checkpointing True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%ls -lh outputs-sft-v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "模型训练结果：\n",
    "- 使用lora训练模型，则保存的lora权重是`adapter_model.bin`, lora配置文件是`adapter_config.json`，合并到base model的方法见`merge_peft_adapter.py`\n",
    "- 日志保存在`output_dir/runs`目录下，可以使用tensorboard查看，启动tensorboard方式如下：`tensorboard --logdir output_dir/runs --host 0.0.0.0 --port 8009`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "lora模型权重合并到base model，合并后的模型保存在`--output_dir`目录下，合并方法如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!python merge_peft_adapter.py --model_type bloom \\\n",
    "    --base_model merged-pt --lora_model outputs-sft-v1 --output_dir ./merged-sft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%ls -lh merged-sft/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%cat merged-sft/config.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Stage2 SFT训练完成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-15T14:07:40.752635Z",
     "start_time": "2023-06-15T14:07:40.731186Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Stage 3: DPO(Direct Preference Optimization)\n",
    "\n",
    "第三阶段：DPO(Direct Preference Optimization)直接偏好优化，DPO通过直接优化语言模型来实现对其行为的精确控制，而无需使用复杂的强化学习，也可以有效学习到人类偏好，DPO相较于RLHF更容易实现且易于训练，效果更好\n",
    "\n",
    "| Stage 3: Direct Preference Optimization        |  [dpo_training.py](https://github.com/shibing624/MedicalGPT/blob/main/dpo_training.py) | [run_dpo.sh](https://github.com/shibing624/MedicalGPT/blob/main/run_dpo.sh)    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 说明：\n",
    "以下 notebook/colab 代码为了快速验证训练代码可用，我们使用了小size的生成模型和小样本数据集，实际使用时，需要使用更大的模型和数据集，以获得更好的效果。\n",
    "\n",
    "1. 生成模型：使用的是Bloom的`bigscience/bloomz-560m` 或者 Stage2得到的SFT模型\n",
    "2. 数据集：DPO阶段使用的是医疗reward数据，抽样了500条，位于`data/reward`文件夹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Stage3 咱们开始吧\n",
    "\n",
    "训练步骤如下：\n",
    "\n",
    "1. 确认训练集\n",
    "2. 执行训练脚本\n",
    "\n",
    "训练脚本的执行逻辑如下：\n",
    "1. 导入依赖包\n",
    "2. 设置参数\n",
    "3. 定义各函数并加载训练集\n",
    "4. 加载模型和tokenizer\n",
    "5. 开始训练并评估\n",
    "6. 查看训练结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%ls ./data/reward/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!python dpo_training.py \\\n",
    "    --model_type bloom \\\n",
    "    --model_name_or_path ./merged-sft \\\n",
    "    --train_file_dir ./data/reward \\\n",
    "    --validation_file_dir ./data/reward \\\n",
    "    --per_device_train_batch_size 3 \\\n",
    "    --per_device_eval_batch_size 1 \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --use_peft True \\\n",
    "    --max_train_samples 1000 \\\n",
    "    --max_eval_samples 10 \\\n",
    "    --max_steps 100 \\\n",
    "    --eval_steps 10 \\\n",
    "    --save_steps 50 \\\n",
    "    --max_source_length 128 \\\n",
    "    --max_target_length 128 \\\n",
    "    --output_dir outputs-dpo-v1 \\\n",
    "    --target_modules all \\\n",
    "    --lora_rank 8 \\\n",
    "    --lora_alpha 16 \\\n",
    "    --lora_dropout 0.05 \\\n",
    "    --torch_dtype float16 \\\n",
    "    --fp16 True \\\n",
    "    --device_map auto \\\n",
    "    --report_to tensorboard \\\n",
    "    --remove_unused_columns False \\\n",
    "    --gradient_checkpointing True \\\n",
    "    --cache_dir ./cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%ls -lh outputs-dpo-v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "模型训练结果：\n",
    "- 使用lora训练模型，则保存的lora权重是`adapter_model.bin`, lora配置文件是`adapter_config.json`，合并到base model的方法见`merge_peft_adapter.py`\n",
    "- 日志保存在`output_dir/runs`目录下，可以使用tensorboard查看，启动tensorboard方式如下：`tensorboard --logdir output_dir/runs --host 0.0.0.0 --port 8009`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "lora模型权重合并到base model，合并后的模型保存在`--output_dir`目录下，合并方法如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!python merge_peft_adapter.py --model_type bloom \\\n",
    "    --base_model merged-sft --lora_model outputs-dpo-v1 --output_dir merged-dpo/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%ls -lh merged-dpo/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%cat merged-dpo/config.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Stage3 偏好建模第一次训练完成。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**至此一个完整的训练流程演示完成。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-26T12:34:29.658428Z",
     "start_time": "2023-06-26T12:34:29.620609Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-26T12:35:00.864463Z",
     "start_time": "2023-06-26T12:34:47.802087Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!python inference.py --model_type bloom --base_model merged-dpo\n",
    "# 或在shell中运行\n",
    "# python inference.py --model_type bloom --base_model merged-dpo --interactive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Input:介绍下南京\n",
    "Response:  南京市位于江苏省西南部，是全国首批历史文化名城、国家中心城市和自由贸易试验区。\n",
    "\n",
    "完。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finetune",
   "language": "python",
   "name": "finetune"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "f34eed0bebedfc4b6ee51ced43d2c030fe3b92f13c149d072205ca200a67b1ec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
