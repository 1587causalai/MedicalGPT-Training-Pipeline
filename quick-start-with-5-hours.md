# Quick Start with 5 Hours

我正在深入研究MedicalGPT项目,特别关注其训练流程的技术实现细节。我的目标是:

1. 理解增量预训练的具体实现方法,包括数据处理、模型架构调整和训练参数设置。
2. 分析有监督微调阶段的数据集构建和训练策略。
3. 深入RLHF的奖励模型设计和PPO算法实现。
4. 探讨DPO方法在该项目中的具体应用和优化。

我计划利用这些知识来开发一个针对个性化LLM研究的定制模型。


## News



- [2024-07-04 10:00] 完成 PPO training pipeline 复现。
- [2024-07-03 22:15] 完成 DPO (Direct Preference Optimization) training pipeline 复现，包括数据处理、模型配置、训练流程和初步评估。

## 直接开发机上跑教程

直接在开发机上跑教程，不需要本地环境配置，省时省力。

## 提示词

```prompt
我正在深入研究**MedicalGPT**项目，特别关注其提供的训练流程，包括增量预训练、有监督微调、RLHF和DPO等阶段。
我计划利用这个项目的代码，来定制一个专门针对个性化LLM研究领域的大语言模型，所以对这个项目的深入理解至关重要。
我希望你扮演一个学习和深入理解此项目的有益助理角色，帮助我理解项目的细节和流程。
```

```prompt
我正在深入研究MedicalGPT项目,特别关注其训练流程的技术实现细节。我的目标是:

1. 理解增量预训练的具体实现方法,包括数据处理、模型架构调整和训练参数设置。
2. 分析有监督微调阶段的数据集构建和训练策略。
3. 深入RLHF的奖励模型设计和PPO算法实现。
4. 探讨DPO方法在该项目中的具体应用和优化。

我计划利用这些知识来开发一个针对个性化LLM研究的定制模型。请协助我详细分析代码实现,并讨论每个阶段的关键技术挑战和解决方案。
```