# Quick Start with 5 Hours

我正在深入研究MedicalGPT项目,特别关注其训练流程的技术实现细节。我的目标是:

1. 理解增量预训练的具体实现方法,包括数据处理、模型架构调整和训练参数设置。
2. 分析有监督微调阶段的数据集构建和训练策略。
3. 深入RLHF的奖励模型设计和PPO算法实现。
4. 探讨DPO方法在该项目中的具体应用和优化。

我计划利用这些知识来开发一个针对个性化LLM研究的定制模型。

## News

- [2024-07-04 10:00] 完成 PPO training pipeline 复现。
- [2024-07-03 22:15] 完成 DPO (Direct Preference Optimization) training pipeline 复现，包括数据处理、模型配置、训练流程和初步评估。

## 直接开发机上跑教程

直接在开发机上跑教程，不需要本地环境配置，省时省力。

## 提示词

```prompt
我正在深入研究**MedicalGPT**项目，特别关注其提供的训练流程，包括增量预训练、有监督微调、RLHF和DPO等阶段。
我计划利用这个项目的代码，来定制一个专门针对个性化LLM研究领域的大语言模型，所以对这个项目的深入理解至关重要。
我希望你扮演一个学习和深入理解此项目的有益助理角色，帮助我理解项目的细节和流程。
```

```prompt
我正在深入研究MedicalGPT项目,特别关注其训练流程的技术实现细节。我的目标是:

1. 理解增量预训练的具体实现方法,包括数据处理、模型架构调整和训练参数设置。
2. 分析有监督微调阶段的数据集构建和训练策略。
3. 深入RLHF的奖励模型设计和PPO算法实现。
4. 探讨DPO方法在该项目中的具体应用和优化。

我计划利用这些知识来开发一个针对个性化LLM研究的定制模型。请协助我详细分析代码实现,并讨论每个阶段的关键技术挑战和解决方案。
```

```prompt
请详细解释奖励模型在代码中的实现细节。特别是，我想了解模型的结构、输入处理方式、分数计算过程、损失函数的设计以及在训练和评估过程中如何使用这些分数。
能否提供一个端到端的示例，展示从输入数据到最终分数输出的完整流程？


我理解了基本的奖励模型结构，现在我想探索如何进一步优化和定制它。例如，如何修改模型结构或损失函数来适应特定领域的需求？
如何处理多轮对话的评分？如何结合外部知识或规则来增强模型的评判能力？您能提供一些高级定制的思路和可能的实现方法吗？
```



我正在开发一个定制的奖励模型训练脚本。我有以下具体需求：

1. 实现一个CustomRewardModel类，它应该包装基础模型，并添加一个评分头（score head），确保输出分数在[0, 1]范围内。

2. 创建一个CustomRewardTrainer类，继承自transformers.Trainer，并实现以下方法：
   - compute_loss：使用arctan损失函数
   - evaluate：可以复用父类的方法
   - prediction_step：需要分别处理chosen和rejected输入
   - save_model：能够保存完整模型，包括PEFT（如果使用）

3. 在主函数中，正确地加载和初始化CustomRewardModel和CustomRewardTrainer。

4. 确保脚本可以直接运行，包含所有必要的导入和函数定义。

请基于这些需求，提供一个完整的、优化过的奖励模型训练脚本。脚本应该清晰、高效，并且易于理解和使用。

